{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/claire/Desktop/Stuff-/codes/dissertation/cxr/rgrg/src/full_model\")\n",
    "sys.path.append(\"/Users/claire/Desktop/Stuff-/codes/dissertation/cxr/rgrg/src\")\n",
    "# import sys\n",
    "sys.path.append(\"/Users/claire/Desktop/Stuff-/codes/dissertation/cxr/rgrg/\")\n",
    "\n",
    "from full_model.train_full_model import *\n",
    "from full_model.generate_reports_for_images import *\n",
    "from full_model.test_set_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0037250518798828125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1173,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5e2d8488fb43fcaf5b2a10212b1f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1173 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_test_dataset, raw_test_2_dataset = get_dataset()\n",
    "\n",
    "def get_tokenized_dataset(tokenizer, raw_test_2_dataset):\n",
    "    def tokenize_function(example):\n",
    "        phrases = example[\"bbox_phrases\"]  # List[str]\n",
    "        bos_token = \"<|endoftext|>\"  # note: in the GPT2 tokenizer, bos_token = eos_token = \"<|endoftext|>\"\n",
    "        eos_token = \"<|endoftext|>\"\n",
    "\n",
    "        phrases_with_special_tokens = [bos_token + phrase + eos_token for phrase in phrases]\n",
    "\n",
    "        # the tokenizer will return input_ids of type List[List[int]] and attention_mask of type List[List[int]]\n",
    "        return tokenizer(phrases_with_special_tokens, truncation=True, max_length=1024)\n",
    "\n",
    "    tokenized_test_2_dataset = raw_test_2_dataset.map(tokenize_function)\n",
    "\n",
    "    # tokenized datasets will consist of the columns\n",
    "    #   - mimic_image_file_path (str)\n",
    "    #   - bbox_coordinates (List[List[int]])\n",
    "    #   - bbox_labels (List[int])\n",
    "    #   - bbox_phrases (List[str])\n",
    "    #   - input_ids (List[List[int]])\n",
    "    #   - attention_mask (List[List[int]])\n",
    "    #   - bbox_phrase_exists (List[bool])\n",
    "    #   - bbox_is_abnormal (List[bool])\n",
    "    #   - reference_report (str)\n",
    "\n",
    "    return tokenized_test_2_dataset\n",
    "\n",
    "# note that we don't actually need to tokenize anything (i.e. we don't need the input ids and attention mask),\n",
    "# because we evaluate the language model on it's generation capabilities (for which we only need the input images)\n",
    "# but since the custom dataset and collator are build in a way that they expect input ids and attention mask\n",
    "# (as they were originally made for training the model),\n",
    "# it's better to just leave it as it is instead of adding unnecessary complexity\n",
    "tokenizer = get_tokenizer()\n",
    "tokenized_test_2_dataset = get_tokenized_dataset(tokenizer,  raw_test_2_dataset)\n",
    "\n",
    "test_transforms = get_transforms()\n",
    "# model = get_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# obj_detector_scores, region_selection_scores, region_abnormal_scores = evaluate_obj_detector_and_binary_classifiers_on_test_set(model, test_loader, test_2_loader)\n",
    "# evaluate_language_model_on_test_set(model, test_loader, test_2_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_name: str, tokenized_dataset, transforms, log):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.transforms = transforms\n",
    "        self.log = log\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get the image_path for potential logging in except block\n",
    "        image_path = self.tokenized_dataset[index][\"mimic_image_file_path\"]\n",
    "\n",
    "        # if something in __get__item fails, then return None\n",
    "        # collate_fn in dataloader filters out None values\n",
    "        try:\n",
    "            bbox_coordinates = self.tokenized_dataset[index][\"bbox_coordinates\"]  # List[List[int]]\n",
    "            bbox_labels = self.tokenized_dataset[index][\"bbox_labels\"]  # List[int]\n",
    "            input_ids = self.tokenized_dataset[index][\"input_ids\"]  # List[List[int]]\n",
    "            attention_mask = self.tokenized_dataset[index][\"attention_mask\"]  # List[List[int]]\n",
    "            bbox_phrase_exists = self.tokenized_dataset[index][\"bbox_phrase_exists\"]  # List[bool]\n",
    "            bbox_is_abnormal = self.tokenized_dataset[index][\"bbox_is_abnormal\"]  # List[bool]\n",
    "\n",
    "            if self.dataset_name != \"train\":\n",
    "                # we only need the reference phrases during evaluation when computing scores for metrics\n",
    "                bbox_phrases = self.tokenized_dataset[index][\"bbox_phrases\"]  # List[str]\n",
    "\n",
    "                # same for the reference_report\n",
    "                reference_report = self.tokenized_dataset[index][\"reference_report\"]  # str\n",
    "\n",
    "            # cv2.imread by default loads an image with 3 channels\n",
    "            # since we have grayscale images, we only have 1 channel and thus use cv2.IMREAD_UNCHANGED to read in the 1 channel\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)[:,:,0]\n",
    "            image = cv2.resize(image, (512, 512))\n",
    "\n",
    "\n",
    "            # apply transformations to image, bbox_coordinates and bbox_labels\n",
    "            transformed = self.transforms(image=image, bboxes=bbox_coordinates, class_labels=bbox_labels)\n",
    "\n",
    "            transformed_image = transformed[\"image\"]\n",
    "\n",
    "            transformed_bbox_coordinates = transformed[\"bboxes\"]\n",
    "            transformed_bbox_labels = transformed[\"class_labels\"]\n",
    "            print(transformed_bbox_coordinates)\n",
    "            transformed_bbox_coordinates = [[x * 2 for x in bbox] for bbox in transformed_bbox_coordinates]\n",
    "            print(transformed_bbox_coordinates)\n",
    "            sample = {\n",
    "                \"image\": transformed_image,\n",
    "                \"bbox_coordinates\": torch.tensor(transformed_bbox_coordinates, dtype=torch.float),\n",
    "                \"bbox_labels\": torch.tensor(transformed_bbox_labels, dtype=torch.int64),\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"bbox_phrase_exists\": torch.tensor(bbox_phrase_exists, dtype=torch.bool),\n",
    "                \"bbox_is_abnormal\": torch.tensor(bbox_is_abnormal, dtype=torch.bool),\n",
    "            }\n",
    "\n",
    "            if self.dataset_name != \"train\":\n",
    "                sample[\"bbox_phrases\"] = bbox_phrases\n",
    "                sample[\"reference_report\"] = reference_report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"__getitem__ failed for: {image_path}\")\n",
    "            self.log.error(f\"Reason: {e}\")\n",
    "            return None\n",
    "\n",
    "        return sample, transformed_bbox_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(42.0, 3.0, 137.0, 130.0), (59.0, 3.0, 133.0, 65.0), (30.0, 114.0, 53.0, 145.0), (46.0, 102.0, 129.0, 137.0), (142.0, 4.0, 219.0, 113.0), (144.0, 9.0, 214.0, 66.0), (142.0, 66.0, 218.0, 83.0), (144.0, 83.0, 219.0, 113.0), (142.0, 62.0, 180.0, 86.0), (148.0, 4.0, 204.0, 38.0), (208.0, 99.0, 230.0, 122.0), (142.0, 90.0, 219.0, 121.0), (123.0, 19.0, 147.0, 75.0), (126.0, 0.0, 154.0, 254.0), (77.0, 5.0, 126.0, 34.0), (149.0, 5.0, 198.0, 34.0), (147.0, 41.0, 170.0, 66.0), (96.0, 14.0, 192.0, 126.0), (114.0, 24.0, 171.0, 69.0), (114.0, 41.0, 147.0, 69.0), (96.0, 70.0, 192.0, 126.0), (96.0, 70.0, 139.0, 89.0), (96.0, 89.0, 139.0, 126.0), (130.0, 67.0, 138.0, 75.0), (32.0, 129.0, 225.0, 254.0)]\n",
      "[[84.0, 6.0, 274.0, 260.0], [118.0, 6.0, 266.0, 130.0], [60.0, 228.0, 106.0, 290.0], [92.0, 204.0, 258.0, 274.0], [284.0, 8.0, 438.0, 226.0], [288.0, 18.0, 428.0, 132.0], [284.0, 132.0, 436.0, 166.0], [288.0, 166.0, 438.0, 226.0], [284.0, 124.0, 360.0, 172.0], [296.0, 8.0, 408.0, 76.0], [416.0, 198.0, 460.0, 244.0], [284.0, 180.0, 438.0, 242.0], [246.0, 38.0, 294.0, 150.0], [252.0, 0.0, 308.0, 508.0], [154.0, 10.0, 252.0, 68.0], [298.0, 10.0, 396.0, 68.0], [294.0, 82.0, 340.0, 132.0], [192.0, 28.0, 384.0, 252.0], [228.0, 48.0, 342.0, 138.0], [228.0, 82.0, 294.0, 138.0], [192.0, 140.0, 384.0, 252.0], [192.0, 140.0, 278.0, 178.0], [192.0, 178.0, 278.0, 252.0], [260.0, 134.0, 276.0, 150.0], [64.0, 258.0, 450.0, 508.0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "254.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CustomDataset(\"test\", tokenized_test_2_dataset.select(range(20)), test_transforms, log)\n",
    "a, b = dataset.__getitem__(0)\n",
    "np.max(b)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
