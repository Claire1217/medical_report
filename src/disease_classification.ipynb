{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "sys.path.append(\"/Users/claire/Desktop/Stuff-/codes/dissertation/cxr/rgrg/src/full_model\")\n",
    "sys.path.append(\"/Users/claire/Desktop/Stuff-/codes/dissertation/cxr/rgrg/src\")\n",
    "# import sys\n",
    "sys.path.append(\"/Users/claire/Desktop/Stuff-/codes/dissertation/cxr/rgrg/\")\n",
    "\n",
    "from full_model.train_full_model import *\n",
    "from full_model.generate_reports_for_images import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    PERCENTAGE_OF_TRAIN_SET_TO_USE = 0.1\n",
    "    PERCENTAGE_OF_VAL_SET_TO_USE = 0.1\n",
    "    usecols = [\n",
    "        \"mimic_image_file_path\",\n",
    "        \"bbox_coordinates\",\n",
    "        \"bbox_labels\",\n",
    "        \"bbox_phrases\",\n",
    "        \"bbox_phrase_exists\",\n",
    "        \"bbox_is_abnormal\",\n",
    "        'bbox_abnormalities',\n",
    "    ]\n",
    "\n",
    "    # all of the columns below are stored as strings in the csv_file\n",
    "    # however, as they are actually lists, we apply the literal_eval func to convert them to lists\n",
    "    converters = {\n",
    "        \"bbox_coordinates\": literal_eval,\n",
    "        \"bbox_labels\": literal_eval,\n",
    "        \"bbox_phrases\": literal_eval,\n",
    "        \"bbox_phrase_exists\": literal_eval,\n",
    "        \"bbox_is_abnormal\": literal_eval,\n",
    "        \"bbox_abnormalities\": literal_eval,\n",
    "    }\n",
    "\n",
    "    datasets_as_dfs = {}\n",
    "    datasets_as_dfs[\"train\"] = pd.read_csv(os.path.join(path_full_dataset, \"train_ab.csv\"), usecols=usecols, converters=converters)\n",
    "    datasets_as_dfs[\"test\"] = pd.read_csv(os.path.join(path_full_dataset, \"test_ab.csv\"), usecols=usecols, converters=converters)\n",
    "\n",
    "    total_num_samples_train = len(datasets_as_dfs[\"train\"])\n",
    "    total_num_samples_val = len(datasets_as_dfs[\"test\"])\n",
    "\n",
    "    # compute new number of samples for both train and val\n",
    "    new_num_samples_train = int(PERCENTAGE_OF_TRAIN_SET_TO_USE * total_num_samples_train)\n",
    "    new_num_samples_val = int(PERCENTAGE_OF_VAL_SET_TO_USE * total_num_samples_val)\n",
    "\n",
    "\n",
    "    from datasets import Dataset\n",
    "    # limit the datasets to those new numbers\n",
    "    datasets_as_dfs[\"train\"] = datasets_as_dfs[\"train\"][:new_num_samples_train]\n",
    "    datasets_as_dfs[\"test\"] = datasets_as_dfs[\"test\"][:new_num_samples_val]\n",
    "\n",
    "    raw_train_dataset = Dataset.from_pandas(datasets_as_dfs[\"train\"])\n",
    "    raw_test_dataset = Dataset.from_pandas(datasets_as_dfs[\"test\"])\n",
    "\n",
    "    return raw_train_dataset, raw_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_name: str, tokenized_dataset, transforms, log):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.transforms = transforms\n",
    "        self.log = log\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get the image_path for potential logging in except block\n",
    "        image_path = self.tokenized_dataset[index][\"mimic_image_file_path\"]\n",
    "\n",
    "        # if something in __get__item fails, then return None\n",
    "        # collate_fn in dataloader filters out None values\n",
    "        try:\n",
    "            bbox_coordinates = self.tokenized_dataset[index][\"bbox_coordinates\"]  # List[List[int]]\n",
    "            bbox_labels = self.tokenized_dataset[index][\"bbox_labels\"]  # List[int]\n",
    "            input_ids = self.tokenized_dataset[index][\"input_ids\"]  # List[List[int]]\n",
    "            attention_mask = self.tokenized_dataset[index][\"attention_mask\"]  # List[List[int]]\n",
    "            bbox_phrase_exists = self.tokenized_dataset[index][\"bbox_phrase_exists\"]  # List[bool]\n",
    "            bbox_is_abnormal = self.tokenized_dataset[index][\"bbox_is_abnormal\"]  # List[bool]\n",
    "            bbox_abnormalities = self.tokenized_dataset[index][\"bbox_abnormalities\"]  # List[List[int]]\n",
    "            \n",
    "\n",
    "            if self.dataset_name != \"train\":\n",
    "                # we only need the reference phrases during evaluation when computing scores for metrics\n",
    "                bbox_phrases = self.tokenized_dataset[index][\"bbox_phrases\"]  # List[str]\n",
    "\n",
    "                # same for the reference_report\n",
    "                reference_report = self.tokenized_dataset[index][\"reference_report\"]  # str\n",
    "\n",
    "            # cv2.imread by default loads an image with 3 channels\n",
    "            # since we have grayscale images, we only have 1 channel and thus use cv2.IMREAD_UNCHANGED to read in the 1 channel\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)[:,:,0]\n",
    "            image = cv2.resize(image, (512, 512))\n",
    "\n",
    "\n",
    "            # apply transformations to image, bbox_coordinates and bbox_labels\n",
    "            transformed = self.transforms(image=image, bboxes=bbox_coordinates, class_labels=bbox_labels)\n",
    "\n",
    "            transformed_image = transformed[\"image\"]\n",
    "\n",
    "            transformed_bbox_coordinates = transformed[\"bboxes\"]\n",
    "            transformed_bbox_labels = transformed[\"class_labels\"]\n",
    "\n",
    "            transformed_bbox_coordinates = [[x * 2 for x in bbox] for bbox in transformed_bbox_coordinates]\n",
    "            sample = {\n",
    "                \"image\": transformed_image,\n",
    "                \"bbox_coordinates\": torch.tensor(transformed_bbox_coordinates, dtype=torch.float),\n",
    "                \"bbox_labels\": torch.tensor(transformed_bbox_labels, dtype=torch.int64),\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"bbox_phrase_exists\": torch.tensor(bbox_phrase_exists, dtype=torch.bool),\n",
    "                \"bbox_is_abnormal\": torch.tensor(bbox_is_abnormal, dtype=torch.bool),\n",
    "                \"bbox_abnormalities\": torch.tensor(bbox_abnormalities, dtype=torch.int64),\n",
    "            }\n",
    "\n",
    "            if self.dataset_name != \"train\":\n",
    "                sample[\"bbox_phrases\"] = bbox_phrases\n",
    "                sample[\"reference_report\"] = reference_report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log.error(f\"__getitem__ failed for: {image_path}\")\n",
    "            self.log.error(f\"Reason: {e}\")\n",
    "            return None\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_datasets(tokenizer, raw_train_dataset):\n",
    "    def tokenize_function(example):\n",
    "        phrases = example[\"bbox_phrases\"]  # List[str]\n",
    "        bos_token = \"<|endoftext|>\"  # note: in the GPT2 tokenizer, bos_token = eos_token = \"<|endoftext|>\"\n",
    "        eos_token = \"<|endoftext|>\"\n",
    "\n",
    "        phrases_with_special_tokens = [bos_token + phrase + eos_token for phrase in phrases]\n",
    "\n",
    "        # the tokenizer will return input_ids of type List[List[int]] and attention_mask of type List[List[int]]\n",
    "        return tokenizer(phrases_with_special_tokens, truncation=True, max_length=1024)\n",
    "\n",
    "    tokenized_train_dataset = raw_train_dataset.map(tokenize_function)\n",
    "\n",
    "    # tokenized datasets will consist of the columns\n",
    "    #   - mimic_image_file_path (str)\n",
    "    #   - bbox_coordinates (List[List[int]])\n",
    "    #   - bbox_labels (List[int])\n",
    "    #   - bbox_phrases (List[str])\n",
    "    #   - input_ids (List[List[int]])\n",
    "    #   - attention_mask (List[List[int]])\n",
    "    #   - bbox_phrase_exists (List[bool])\n",
    "    #   - bbox_is_abnormal (List[bool])\n",
    "    #\n",
    "    #   val dataset will have additional column:\n",
    "    #   - reference_report (str)\n",
    "\n",
    "    return tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_name: str, tokenized_dataset, transforms, log):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.transforms = transforms\n",
    "        self.log = log\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "   \n",
    "        # get the image_path for potential logging in except block\n",
    "        image_path = self.tokenized_dataset[index][\"mimic_image_file_path\"]\n",
    "\n",
    "        # if something in __get__item fails, then return None\n",
    "        # collate_fn in dataloader filters out None values\n",
    "        bbox_coordinates = self.tokenized_dataset[index][\"bbox_coordinates\"]  # List[List[int]]\n",
    "        bbox_labels = self.tokenized_dataset[index][\"bbox_labels\"]  # List[int]\n",
    "        input_ids = self.tokenized_dataset[index][\"input_ids\"]  # List[List[int]]\n",
    "        attention_mask = self.tokenized_dataset[index][\"attention_mask\"]  # List[List[int]]\n",
    "        bbox_phrase_exists = self.tokenized_dataset[index][\"bbox_phrase_exists\"]  # List[bool]\n",
    "        bbox_is_abnormal = self.tokenized_dataset[index][\"bbox_is_abnormal\"]  # List[bool]\n",
    "        bbox_abnormalities = self.tokenized_dataset[index][\"bbox_abnormalities\"]  # List[List[int]]\n",
    "\n",
    "\n",
    "        # if self.dataset_name != \"train\":\n",
    "        #     # we only need the reference phrases during evaluation when computing scores for metrics\n",
    "        #     bbox_phrases = self.tokenized_dataset[index][\"bbox_phrases\"]  # List[str]\n",
    "\n",
    "        #     # same for the reference_report\n",
    "        #     reference_report = self.tokenized_dataset[index][\"reference_report\"]  # str\n",
    "\n",
    "        # cv2.imread by default loads an image with 3 channels\n",
    "        # since we have grayscale images, we only have 1 channel and thus use cv2.IMREAD_UNCHANGED to read in the 1 channel\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)[:,:,0]\n",
    "        image = cv2.resize(image, (512, 512))\n",
    "\n",
    "\n",
    "        # apply transformations to image, bbox_coordinates and bbox_labels\n",
    "        transformed = self.transforms(image=image, bboxes=bbox_coordinates, class_labels=bbox_labels)\n",
    "\n",
    "        transformed_image = transformed[\"image\"]\n",
    "\n",
    "        transformed_bbox_coordinates = transformed[\"bboxes\"]\n",
    "        transformed_bbox_labels = transformed[\"class_labels\"]\n",
    "     \n",
    "        transformed_bbox_coordinates = [[x * 2 for x in bbox] for bbox in transformed_bbox_coordinates]\n",
    "        sample = {\n",
    "            \"image\": transformed_image,\n",
    "            \"bbox_coordinates\": torch.tensor(transformed_bbox_coordinates, dtype=torch.float),\n",
    "            \"bbox_labels\": torch.tensor(transformed_bbox_labels, dtype=torch.int64),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"bbox_phrase_exists\": torch.tensor(bbox_phrase_exists, dtype=torch.bool),\n",
    "            \"bbox_is_abnormal\": torch.tensor(bbox_is_abnormal, dtype=torch.bool),\n",
    "            \"bbox_abnormalities\": bbox_abnormalities,\n",
    "        }\n",
    "    \n",
    "        # if self.dataset_name != \"train\":\n",
    "        #     sample[\"bbox_phrases\"] = bbox_phrases\n",
    "        #     sample[\"reference_report\"] = reference_report\n",
    "\n",
    "\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING]: Parameter 'function'=<function get_tokenized_datasets.<locals>.tokenize_function at 0x2fece1820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008011817932128906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 11391,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1863231256324908b152daab3b7e2ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11391 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_train_dataset, raw_test_dataset = get_datasets()\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "# tokenize the raw datasets\n",
    "tokenized_train_dataset= get_tokenized_datasets(tokenizer, raw_train_dataset)\n",
    "train_transforms = get_transforms(\"train\")\n",
    "train_dataset_complete = CustomDataset(\"train\", tokenized_train_dataset, train_transforms, log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[-1.5466, -1.5466, -1.5466,  ..., -1.5466, -1.5466, -1.5466],\n",
       "          [-1.5466, -1.5466, -1.5466,  ..., -1.5466, -1.5466, -1.5466],\n",
       "          [-1.5466, -1.5466, -1.5466,  ..., -1.5466, -1.5466, -1.5466],\n",
       "          ...,\n",
       "          [ 0.9985,  0.9985,  0.9855,  ..., -1.0532, -1.0662, -1.0791],\n",
       "          [ 1.0375,  1.0375,  1.0504,  ..., -1.1311, -1.1960, -1.2350],\n",
       "          [ 1.0634,  1.0764,  1.0894,  ..., -1.1571, -1.2609, -1.3129]]]),\n",
       " 'bbox_coordinates': tensor([[ 82.,  18., 242., 294.],\n",
       "         [128.,  26., 242., 116.],\n",
       "         [112., 116., 234., 180.],\n",
       "         [ 82., 180., 224., 294.],\n",
       "         [172., 102., 236., 192.],\n",
       "         [136.,  18., 240.,  80.],\n",
       "         [ 58., 244., 104., 290.],\n",
       "         [ 82., 246., 256., 294.],\n",
       "         [268.,  26., 428., 292.],\n",
       "         [268.,  34., 386., 116.],\n",
       "         [268., 116., 394., 180.],\n",
       "         [268., 180., 428., 292.],\n",
       "         [268., 102., 330., 192.],\n",
       "         [272.,  26., 374.,  80.],\n",
       "         [404., 260., 450., 306.],\n",
       "         [268., 258., 428., 292.],\n",
       "         [214.,   8., 278., 172.],\n",
       "         [208.,   0., 284., 508.],\n",
       "         [108.,  40., 230.,  76.],\n",
       "         [272.,  38., 456.,  80.],\n",
       "         [252.,  80., 300., 116.],\n",
       "         [188.,  52., 364., 292.],\n",
       "         [204.,  58., 304., 168.],\n",
       "         [204.,  80., 252., 168.],\n",
       "         [188., 170., 364., 292.],\n",
       "         [188., 170., 244., 210.],\n",
       "         [188., 210., 244., 290.],\n",
       "         [236., 108., 258., 130.],\n",
       "         [ 82., 258., 428., 508.]]),\n",
       " 'bbox_labels': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " 'input_ids': [[50256,\n",
       "   32,\n",
       "   46963,\n",
       "   45912,\n",
       "   287,\n",
       "   262,\n",
       "   826,\n",
       "   12317,\n",
       "   40167,\n",
       "   318,\n",
       "   649,\n",
       "   422,\n",
       "   3161,\n",
       "   12452,\n",
       "   13,\n",
       "   383,\n",
       "   17675,\n",
       "   286,\n",
       "   262,\n",
       "   21726,\n",
       "   389,\n",
       "   1598,\n",
       "   13,\n",
       "   6498,\n",
       "   6727,\n",
       "   49918,\n",
       "   35647,\n",
       "   393,\n",
       "   2347,\n",
       "   13,\n",
       "   1086,\n",
       "   403,\n",
       "   889,\n",
       "   286,\n",
       "   24537,\n",
       "   1575,\n",
       "   2522,\n",
       "   918,\n",
       "   291,\n",
       "   18333,\n",
       "   11,\n",
       "   826,\n",
       "   3744,\n",
       "   621,\n",
       "   1364,\n",
       "   11,\n",
       "   743,\n",
       "   307,\n",
       "   9233,\n",
       "   284,\n",
       "   1402,\n",
       "   914,\n",
       "   15880,\n",
       "   13,\n",
       "   2102,\n",
       "   11,\n",
       "   1813,\n",
       "   826,\n",
       "   17570,\n",
       "   1336,\n",
       "   1108,\n",
       "   11,\n",
       "   257,\n",
       "   2347,\n",
       "   7186,\n",
       "   287,\n",
       "   1281,\n",
       "   12,\n",
       "   672,\n",
       "   7249,\n",
       "   425,\n",
       "   35647,\n",
       "   318,\n",
       "   1626,\n",
       "   262,\n",
       "   22577,\n",
       "   13,\n",
       "   50256],\n",
       "  [50256, 11028, 6727, 49918, 35647, 393, 2347, 13, 50256],\n",
       "  [50256, 11028, 6727, 49918, 35647, 393, 2347, 13, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256,\n",
       "   1858,\n",
       "   318,\n",
       "   635,\n",
       "   1336,\n",
       "   1108,\n",
       "   286,\n",
       "   262,\n",
       "   826,\n",
       "   289,\n",
       "   346,\n",
       "   388,\n",
       "   543,\n",
       "   318,\n",
       "   649,\n",
       "   13,\n",
       "   2102,\n",
       "   11,\n",
       "   1813,\n",
       "   826,\n",
       "   17570,\n",
       "   1336,\n",
       "   1108,\n",
       "   11,\n",
       "   257,\n",
       "   2347,\n",
       "   7186,\n",
       "   287,\n",
       "   1281,\n",
       "   12,\n",
       "   672,\n",
       "   7249,\n",
       "   425,\n",
       "   35647,\n",
       "   318,\n",
       "   1626,\n",
       "   262,\n",
       "   22577,\n",
       "   13,\n",
       "   50256],\n",
       "  [50256,\n",
       "   32,\n",
       "   46963,\n",
       "   45912,\n",
       "   287,\n",
       "   262,\n",
       "   826,\n",
       "   12317,\n",
       "   40167,\n",
       "   318,\n",
       "   649,\n",
       "   422,\n",
       "   3161,\n",
       "   12452,\n",
       "   13,\n",
       "   50256],\n",
       "  [50256,\n",
       "   3629,\n",
       "   403,\n",
       "   889,\n",
       "   286,\n",
       "   24537,\n",
       "   1575,\n",
       "   2522,\n",
       "   918,\n",
       "   291,\n",
       "   18333,\n",
       "   11,\n",
       "   826,\n",
       "   3744,\n",
       "   621,\n",
       "   1364,\n",
       "   11,\n",
       "   743,\n",
       "   307,\n",
       "   9233,\n",
       "   284,\n",
       "   1402,\n",
       "   914,\n",
       "   15880,\n",
       "   13,\n",
       "   50256],\n",
       "  [50256, 50256],\n",
       "  [50256,\n",
       "   464,\n",
       "   17675,\n",
       "   286,\n",
       "   262,\n",
       "   21726,\n",
       "   389,\n",
       "   1598,\n",
       "   13,\n",
       "   1086,\n",
       "   403,\n",
       "   889,\n",
       "   286,\n",
       "   24537,\n",
       "   1575,\n",
       "   2522,\n",
       "   918,\n",
       "   291,\n",
       "   18333,\n",
       "   11,\n",
       "   826,\n",
       "   3744,\n",
       "   621,\n",
       "   1364,\n",
       "   11,\n",
       "   743,\n",
       "   307,\n",
       "   9233,\n",
       "   284,\n",
       "   1402,\n",
       "   914,\n",
       "   15880,\n",
       "   13,\n",
       "   50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256,\n",
       "   3629,\n",
       "   403,\n",
       "   889,\n",
       "   286,\n",
       "   24537,\n",
       "   1575,\n",
       "   2522,\n",
       "   918,\n",
       "   291,\n",
       "   18333,\n",
       "   11,\n",
       "   826,\n",
       "   3744,\n",
       "   621,\n",
       "   1364,\n",
       "   11,\n",
       "   743,\n",
       "   307,\n",
       "   9233,\n",
       "   284,\n",
       "   1402,\n",
       "   914,\n",
       "   15880,\n",
       "   13,\n",
       "   50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 464, 2612, 2546, 318, 1353, 3487, 13, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256],\n",
       "  [50256, 50256]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1]],\n",
       " 'bbox_phrase_exists': tensor([ True,  True,  True, False,  True,  True,  True, False,  True, False,\n",
       "         False, False, False, False,  True, False, False, False, False, False,\n",
       "         False, False, False, False,  True, False, False, False, False]),\n",
       " 'bbox_is_abnormal': tensor([ True,  True,  True, False,  True,  True,  True, False,  True, False,\n",
       "         False, False, False, False,  True, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False]),\n",
       " 'bbox_abnormalities': [[21, 0, 11, 12, 17],\n",
       "  [21, 0, 17],\n",
       "  [21, 0, 17],\n",
       "  [],\n",
       "  [6, 21, 0, 17],\n",
       "  [0],\n",
       "  [11, 12, 0],\n",
       "  [],\n",
       "  [0, 11, 12],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [11, 12, 0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_complete.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0036439895629882812,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3080,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076c871c827144dca089b0e3039f194c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3080 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_val_dataset = get_tokenized_datasets(tokenizer, raw_test_dataset)\n",
    "val_transforms = get_transforms(\"val\")\n",
    "val_dataset_complete = CustomDataset(\"val\", tokenized_val_dataset, val_transforms, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. feature data loader\n",
    "feature, anatomical_ind, finding_labels = next(data_loader)\n",
    "fetaure: 1 * 1024\n",
    "anatomical_ind: 1\n",
    "finding_labels: [0, 4, 19]\n",
    "\n",
    "2. model\n",
    "a. 29 models for each anatomical region\n",
    "b. one model for all anatomical region, 40-class classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/Users/claire/Desktop/Stuff-/codes/dissertation/data/checkpoints/full_model_checkpoint_val_loss_19.793_overall_steps_155252.pt\"\n",
    "model = get_model(checkpoint_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_complete = CustomDataset(\"train\", tokenized_train_dataset, train_transforms, log)\n",
    "val_dataset_complete = CustomDataset(\"val\", tokenized_val_dataset, val_transforms, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loaders(tokenizer, train_dataset_complete, val_dataset_complete):\n",
    "    custom_collate_test = CustomCollator(\n",
    "        tokenizer=tokenizer, is_val_or_test=True, pretrain_without_lm_model=False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        train_dataset_complete,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset_complete,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return test_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = []\n",
    "one_hot_abs= []\n",
    "sample = train_dataset_complete[0]\n",
    "image = sample['image']\n",
    "image = image.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, feature,_=  model.object_detector(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append(feature)\n",
    "bbox_abnormalities = sample['bbox_abnormalities']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(lst):\n",
    "    lst = [0 if x not in lst else 1 for x in range(41)]\n",
    "    return lst\n",
    "one_hot_ab = torch.Tensor([one_hot(x) for x in bbox_abnormalities])\n",
    "one_hot_abs.append(one_hot_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.0382,  0.0046,  0.0988,  ...,  0.0264,  0.2625, -0.1633],\n",
       "          [-0.2999,  0.1143, -0.1615,  ...,  0.0610,  0.2086, -0.2958],\n",
       "          [-0.1840, -0.1034,  0.1876,  ...,  0.1570,  0.2957, -0.3913],\n",
       "          ...,\n",
       "          [ 0.2341,  0.0404, -0.6592,  ...,  0.4296,  0.0581,  0.2007],\n",
       "          [-0.2065,  0.0582, -0.2670,  ...,  0.0558, -0.0731, -0.2660],\n",
       "          [ 0.1962,  0.3054,  0.1338,  ...,  0.0078,  0.0516,  0.0147]]],\n",
       "        grad_fn=<ViewBackward0>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset_complete[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# random tensor of shape 100, 512, 512\n",
    "tmp = torch.rand(100,1, 512, 512)\n",
    "model.object_detector(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_abs= []\n",
    "for sample in dataset:\n",
    "    image = sample['image']\n",
    "    image = image.unsqueeze(0)\n",
    "    _, _, feature,_=  model.object_detector(image)\n",
    "    features.append(feature)\n",
    "    bbox_abnormalities = sample['bbox_abnormalities']\n",
    "    one_hot_ab = torch.Tensor([one_hot(x) for x in bbox_abnormalities])\n",
    "    one_hot_abs.append(one_hot_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def initial_features(dataset, model):\n",
    "    def one_hot(lst):\n",
    "        lst = [0 if x not in lst else 1 for x in range(41)]\n",
    "        return lst\n",
    "    features = []\n",
    "    one_hot_abs= []\n",
    "    for sample in dataset:\n",
    "        print(1)\n",
    "        image = sample['image']\n",
    "        image = image.unsqueeze(0)\n",
    "        _, _, feature,_=  model.object_detector(image)\n",
    "        features.append(feature)\n",
    "        bbox_abnormalities = sample['bbox_abnormalities']\n",
    "        one_hot_ab = torch.Tensor([one_hot(x) for x in bbox_abnormalities])\n",
    "        one_hot_abs.append(one_hot_ab)\n",
    "    return features, one_hot_abs\n",
    "\n",
    "# save the features and one_hot_abs\n",
    "\n",
    "val_features, val_one_hot_abs = initial_features(val_dataset_complete, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_one_hot_abs = initial_features(train_dataset_complete, model)\n",
    "# save the features and one_hot_abs\n",
    "torch.save(train_features, '/Users/claire/Desktop/Stuff-/codes/dissertation/data/train_features.pt')\n",
    "torch.save(train_one_hot_abs, '/Users/claire/Desktop/Stuff-/codes/dissertation/data/train_one_hot_abs.pt')\n",
    "torch.save(val_features, '/Users/claire/Desktop/Stuff-/codes/dissertation/data/val_features.pt')\n",
    "torch.save(val_one_hot_abs, '/Users/claire/Desktop/Stuff-/codes/dissertation/data/val_one_hot_abs.pt')\n",
    "\n",
    "# load the features and one_hot_abs\n",
    "train_features = torch.load('/Users/claire/Desktop/Stuff-/codes/dissertation/data/train_features.pt')\n",
    "train_one_hot_abs = torch.load('/Users/claire/Desktop/Stuff-/codes/dissertation/data/train_one_hot_abs.pt')\n",
    "val_features = torch.load('/Users/claire/Desktop/Stuff-/codes/dissertation/data/val_features.pt')\n",
    "val_one_hot_abs = torch.load('/Users/claire/Desktop/Stuff-/codes/dissertation/data/val_one_hot_abs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, customer_dataset, model, mixup=True):\n",
    "        self.dataset = customer_dataset\n",
    "        self.model = model\n",
    "        self.mixup = mixup\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    '''\n",
    "    Returns a list of tuples of the form (top_region_features, region_id, finding_id)\n",
    "    An index co\n",
    "    '''\n",
    "    def one_hot(self, lst):\n",
    "        lst = [0 if x not in lst else 1 for x in range(41)]\n",
    "        return lst\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        image = sample['image']\n",
    "        image = image.unsqueeze(0)\n",
    "        _, _, top_region_features,_= self.model.object_detector(image)\n",
    "        bbox_abnormalities = sample['bbox_abnormalities']\n",
    "        one_hot_ab = [self.one_hot(x) for x in bbox_abnormalities]\n",
    "        return top_region_features, one_hot_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, customer_dataset, model):\n",
    "        self.dataset = customer_dataset\n",
    "        self.model = model\n",
    "        features, one_hot_abs = self.initial_features(customer_dataset, model)\n",
    "        self.features = features\n",
    "        self.one_hot_abs = one_hot_abs\n",
    "\n",
    "    \n",
    "    def initial_features(self, dataset, model):\n",
    "        def one_hot(lst):\n",
    "            lst = [0 if x not in lst else 1 for x in range(41)]\n",
    "            return lst\n",
    "        features = []\n",
    "        one_hot_abs= []\n",
    "        for sample in dataset:\n",
    "            image = sample['image']\n",
    "            image = image.unsqueeze(0)\n",
    "            _, _, feature,_=  model.object_detector(image)\n",
    "            features.append(feature)\n",
    "            bbox_abnormalities = sample['bbox_abnormalities']\n",
    "            one_hot_ab = torch.Tensor([one_hot(x) for x in bbox_abnormalities])\n",
    "            one_hot_abs.append(one_hot_ab)\n",
    "        return features, one_hot_abs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    '''\n",
    "    Returns a list of tuples of the form (top_region_features, region_id, finding_id)\n",
    "    An index co\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        one_hot_ab = self.one_hot_abs[index]\n",
    "        return feature, one_hot_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nn\n",
    "import torch.nn as nn\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(29*1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 29*41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.view(-1, 29, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assume you have split your dataset into training and validation sets\n",
    "train_dataset = FeatureDataset(train_dataset_complete, model)\n",
    "val_dataset = FeatureDataset(val_dataset_complete, model)\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0717, -0.0461,  0.1126,  ..., -0.0140,  0.0137, -0.1753],\n",
       "         [-0.2667, -0.0901, -0.0809,  ...,  0.0708, -0.0802, -0.2500],\n",
       "         [-0.2707, -0.0151,  0.2605,  ..., -0.1532, -0.0634, -0.4341],\n",
       "         ...,\n",
       "         [ 0.3879,  0.3287, -0.5041,  ...,  0.5047,  0.0249,  0.2009],\n",
       "         [-0.0717, -0.0461,  0.1126,  ..., -0.0140,  0.0137, -0.1753],\n",
       "         [ 0.2453,  0.3405,  0.2579,  ..., -0.0862,  0.1149,  0.0326]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader.dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    print('start training')\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        print(batch)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "start training\n",
      "11391\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "classifier_model = MultiClassClassifier()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train and test the model_\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, classifier_model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = FeatureDataset(train_dataset_complete, model)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29, 1024]) torch.Size([29, 41])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m dataset \u001b[39m=\u001b[39m FeatureDataset(train_dataset_complete, model)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset)):\n\u001b[0;32m---> 11\u001b[0m     lst \u001b[39m=\u001b[39m dataset[i]\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m a, b, c \u001b[39min\u001b[39;00m lst:\n\u001b[1;32m     13\u001b[0m         X\u001b[39m.\u001b[39mappend(a\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()) \u001b[39m# convert tensor to numpy array\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mFeatureDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m image \u001b[39m=\u001b[39m sample[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m _, _, top_region_features,_\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mobject_detector(image)\n\u001b[1;32m     19\u001b[0m lst \u001b[39m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m bbox_abnormalities \u001b[39m=\u001b[39m sample[\u001b[39m'\u001b[39m\u001b[39mbbox_abnormalities\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Stuff-/codes/dissertation/cxr/rgrg/src/object_detector/object_detector.py:224\u001b[0m, in \u001b[0;36mObjectDetector.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    221\u001b[0m images, features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_inputs_for_rpn_and_roi(images, features)\n\u001b[1;32m    223\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m--> 224\u001b[0m roi_heads_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[1;32m    226\u001b[0m \u001b[39m# the roi_heads_output always includes the detector_losses\u001b[39;00m\n\u001b[1;32m    227\u001b[0m detector_losses \u001b[39m=\u001b[39m roi_heads_output[\u001b[39m\"\u001b[39m\u001b[39mdetector_losses\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Stuff-/codes/dissertation/cxr/rgrg/src/object_detector/custom_roi_heads.py:233\u001b[0m, in \u001b[0;36mCustomRoIHeads.forward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    229\u001b[0m     regression_targets \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# box_roi_pool_feature_maps has shape [overall_num_proposals_for_all_images x 2048 x 8 x 8]\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# 4 * 2048 * 16 * 16\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m box_roi_pool_feature_maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_roi_pool(features, proposals, image_shapes)\n\u001b[1;32m    234\u001b[0m \u001b[39m# box_feature_vectors has shape [overall_num_proposals_for_all_images x 1024]\u001b[39;00m\n\u001b[1;32m    235\u001b[0m box_feature_vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_head(box_roi_pool_feature_maps) \u001b[39m# 491 * 1024 * 1 * 1 \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/ops/poolers.py:331\u001b[0m, in \u001b[0;36mMultiScaleRoIAlign.forward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_levels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_levels \u001b[39m=\u001b[39m _setup_scales(\n\u001b[1;32m    328\u001b[0m         x_filtered, image_shapes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonical_scale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonical_level\n\u001b[1;32m    329\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m _multiscale_roi_align(\n\u001b[1;32m    332\u001b[0m     x_filtered,\n\u001b[1;32m    333\u001b[0m     boxes,\n\u001b[1;32m    334\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size,\n\u001b[1;32m    335\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampling_ratio,\n\u001b[1;32m    336\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscales,\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap_levels,\n\u001b[1;32m    338\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/ops/poolers.py:176\u001b[0m, in \u001b[0;36m_multiscale_roi_align\u001b[0;34m(x_filtered, boxes, output_size, sampling_ratio, scales, mapper)\u001b[0m\n\u001b[1;32m    173\u001b[0m rois \u001b[39m=\u001b[39m _convert_to_roi_format(boxes)\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m num_levels \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mreturn\u001b[39;00m roi_align(\n\u001b[1;32m    177\u001b[0m         x_filtered[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m         rois,\n\u001b[1;32m    179\u001b[0m         output_size\u001b[39m=\u001b[39;49moutput_size,\n\u001b[1;32m    180\u001b[0m         spatial_scale\u001b[39m=\u001b[39;49mscales[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         sampling_ratio\u001b[39m=\u001b[39;49msampling_ratio,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    184\u001b[0m levels \u001b[39m=\u001b[39m mapper(boxes)\n\u001b[1;32m    186\u001b[0m num_rois \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(rois)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/ops/roi_align.py:61\u001b[0m, in \u001b[0;36mroi_align\u001b[0;34m(input, boxes, output_size, spatial_scale, sampling_ratio, aligned)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(rois, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     60\u001b[0m     rois \u001b[39m=\u001b[39m convert_boxes_to_roi_format(rois)\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mtorchvision\u001b[39m.\u001b[39;49mroi_align(\n\u001b[1;32m     62\u001b[0m     \u001b[39minput\u001b[39;49m, rois, spatial_scale, output_size[\u001b[39m0\u001b[39;49m], output_size[\u001b[39m1\u001b[39;49m], sampling_ratio, aligned\n\u001b[1;32m     63\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/_ops.py:143\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Prepare your data\n",
    "X = [] # feature vectors\n",
    "region_ids = [] # region ids\n",
    "y = [] # disease labels\n",
    "dataset = FeatureDataset(train_dataset_complete, model)\n",
    "for i in range(len(dataset)):\n",
    "    lst = dataset[i]\n",
    "    for a, b, c in lst:\n",
    "        X.append(a.detach().numpy()) # convert tensor to numpy array\n",
    "        region_ids.append(b)\n",
    "        y.append(c)\n",
    "\n",
    "# One-hot encode region ids\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "region_ids = encoder.fit_transform(np.array(region_ids).reshape(-1, 1))\n",
    "\n",
    "# Concatenate feature vectors and region ids\n",
    "X = np.concatenate((X, region_ids), axis=1)\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode region ids\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "region_ids = encoder.fit_transform(np.array(region_ids).reshape(-1, 1))\n",
    "\n",
    "# Concatenate feature vectors and region ids\n",
    "X = np.concatenate((X, region_ids), axis=1)\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class DiseaseClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiseaseClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, len(np.unique(y_train))) # number of unique disease labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Convert your data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize your model\n",
    "nn_model = DiseaseClassifier()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0003)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(torch.tensor([[9, 0.2, 0.3]]), torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6581358909606934\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.518080472946167\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9580976963043213\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9304499626159668\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4994324445724487\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8277125358581543\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.553109049797058\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9471303224563599\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6802353858947754\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7639497518539429\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7882554531097412\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4971253871917725\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5017262697219849\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.661021113395691\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6185113191604614\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5908167362213135\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6933313608169556\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8152809143066406\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5893968343734741\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6848132610321045\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.767681360244751\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7817497253417969\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.706479549407959\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4721823930740356\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.0730793476104736\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7585384845733643\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7939709424972534\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7427161931991577\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6481082439422607\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9218741655349731\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6891227960586548\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5159032344818115\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6600111722946167\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8596936464309692\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4200749397277832\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.861459732055664\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7161108255386353\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.585886836051941\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.66111159324646\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6065294742584229\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7967368364334106\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6756242513656616\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6289833784103394\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7381917238235474\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6489084959030151\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8093081712722778\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6346044540405273\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8676584959030151\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.58379328250885\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4301707744598389\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.508978247642517\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.0344653129577637\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6857210397720337\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.579006314277649\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6283246278762817\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6675359010696411\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6681164503097534\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.17590594291687\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7336516380310059\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5929616689682007\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.629095196723938\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5905603170394897\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.910436987876892\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5603700876235962\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6355229616165161\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8973430395126343\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.845171570777893\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8453631401062012\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7417384386062622\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4604302644729614\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7134114503860474\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.578979253768921\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.696522831916809\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5633244514465332\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9594157934188843\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.839705228805542\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.793421983718872\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9225246906280518\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7785805463790894\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9378291368484497\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6702793836593628\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7207213640213013\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5569064617156982\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.678162693977356\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8095322847366333\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.408597707748413\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8086365461349487\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.732954502105713\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6881191730499268\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5737422704696655\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7672580480575562\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.684484839439392\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9291404485702515\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7230242490768433\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5979905128479004\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6476904153823853\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.781640648841858\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6456103324890137\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.808369517326355\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6958732604980469\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.3672183752059937\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6464017629623413\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.871415138244629\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7751587629318237\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.505326747894287\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7162754535675049\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8486666679382324\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8231022357940674\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6495506763458252\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9169241189956665\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5559622049331665\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8433595895767212\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7379062175750732\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6944129467010498\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5634725093841553\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7887651920318604\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5945992469787598\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9031548500061035\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6680699586868286\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7428374290466309\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6145048141479492\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6235244274139404\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4729161262512207\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7468310594558716\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5273849964141846\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6237331628799438\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8452972173690796\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4849467277526855\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9798377752304077\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6118602752685547\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.517534852027893\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7395058870315552\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9831290245056152\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5268577337265015\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4466818571090698\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5753685235977173\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5059846639633179\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8404959440231323\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.732295274734497\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5695003271102905\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5980788469314575\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7947869300842285\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6639889478683472\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.525404691696167\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5136502981185913\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4661750793457031\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6366270780563354\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5756707191467285\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7481337785720825\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.511398434638977\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4922723770141602\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.696452260017395\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7080508470535278\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4119642972946167\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5652730464935303\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5234776735305786\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.073204278945923\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5687967538833618\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5566887855529785\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6204081773757935\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5539246797561646\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7108783721923828\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6144131422042847\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5006619691848755\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7795186042785645\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7401822805404663\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.650594711303711\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5038336515426636\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.2146425247192383\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7166262865066528\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4981367588043213\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6360102891921997\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.674553394317627\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7038774490356445\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.3542226552963257\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7943789958953857\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5526893138885498\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5926358699798584\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.480882167816162\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5996203422546387\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7751327753067017\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7865567207336426\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5278573036193848\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8110864162445068\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9417599439620972\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8144381046295166\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.835735559463501\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6785738468170166\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.779999017715454\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5818363428115845\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7082117795944214\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7642405033111572\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7885180711746216\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7156925201416016\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4186793565750122\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5987210273742676\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6009835004806519\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7499899864196777\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4645549058914185\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.489170789718628\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6542199850082397\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7961417436599731\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.690989375114441\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.433543086051941\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8397581577301025\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6170134544372559\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8493727445602417\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7272597551345825\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6306076049804688\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7422999143600464\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.432076334953308\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.605965256690979\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.465575098991394\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8208160400390625\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6850866079330444\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5633437633514404\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5718995332717896\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6922301054000854\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5258324146270752\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.587947964668274\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5368963479995728\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.782473087310791\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8041361570358276\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5894478559494019\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5883346796035767\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6074244976043701\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6338342428207397\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7045459747314453\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.38480806350708\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5981166362762451\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8297313451766968\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5667896270751953\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5032075643539429\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6107741594314575\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6967390775680542\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5699716806411743\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6160475015640259\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9724578857421875\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6038916110992432\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7272764444351196\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5367416143417358\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.561730146408081\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5776139497756958\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5808823108673096\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8257405757904053\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4835519790649414\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9361342191696167\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.715696096420288\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.854361891746521\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8697093725204468\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.918886423110962\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4372268915176392\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6781842708587646\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.616856336593628\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.468037486076355\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5495771169662476\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.3666167259216309\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.794075608253479\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5231797695159912\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.556045413017273\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8136696815490723\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6342682838439941\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6579184532165527\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9161680936813354\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5426392555236816\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.814223051071167\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6075867414474487\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6255367994308472\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6855987310409546\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5379912853240967\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5952049493789673\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.743047833442688\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7687854766845703\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4241851568222046\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8805116415023804\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9262254238128662\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7237592935562134\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5055283308029175\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5294978618621826\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8535274267196655\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6774885654449463\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5474869012832642\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7020273208618164\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4814881086349487\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7756319046020508\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.732409119606018\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5177565813064575\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7509081363677979\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8387852907180786\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7145706415176392\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.479063630104065\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6409590244293213\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.434874415397644\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6365481615066528\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6859636306762695\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7630807161331177\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.717835545539856\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.569344401359558\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.666541576385498\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6693742275238037\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5877137184143066\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.717334508895874\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.814969539642334\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7064274549484253\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.0137572288513184\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4799429178237915\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.856756329536438\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.604217290878296\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6599040031433105\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.52655827999115\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.888548493385315\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4630459547042847\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7137068510055542\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7578645944595337\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4522475004196167\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7250444889068604\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5379767417907715\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6406965255737305\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7846482992172241\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.771378993988037\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5806134939193726\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7348281145095825\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8244448900222778\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7119303941726685\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.654300332069397\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5170097351074219\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.792054295539856\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6038869619369507\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9695522785186768\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4822609424591064\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.263778567314148\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4620740413665771\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4405168294906616\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5518224239349365\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.691385269165039\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6070528030395508\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7397862672805786\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.56626296043396\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8975279331207275\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5405844449996948\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7512953281402588\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.734718680381775\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8955458402633667\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.83624267578125\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4884408712387085\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7540606260299683\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.707392692565918\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9391999244689941\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7114053964614868\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5736240148544312\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7146825790405273\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7911800146102905\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.802962303161621\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.476570725440979\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6079355478286743\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.573561429977417\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6874926090240479\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6585184335708618\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.801715612411499\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8338367938995361\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5618371963500977\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5437779426574707\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7358572483062744\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.600717306137085\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8037110567092896\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.85080885887146\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7128205299377441\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6752455234527588\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8246947526931763\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5680445432662964\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6434922218322754\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7752947807312012\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6889315843582153\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5954978466033936\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8318387269973755\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7167255878448486\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.76505708694458\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8112549781799316\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6085227727890015\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7508667707443237\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7559386491775513\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4504121541976929\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7751822471618652\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6654094457626343\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7957468032836914\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7603919506072998\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5647717714309692\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.592439889907837\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5333646535873413\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.946746826171875\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5945388078689575\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6815030574798584\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8590539693832397\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6098723411560059\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.3101688623428345\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.859392523765564\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.600484013557434\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6784974336624146\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8577123880386353\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.763637661933899\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6346369981765747\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.779101848602295\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5788073539733887\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7761057615280151\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6316744089126587\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.538859248161316\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.654644250869751\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6578073501586914\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7221708297729492\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6404876708984375\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5593935251235962\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.562848687171936\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8265910148620605\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.473031997680664\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6619395017623901\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7966779470443726\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4695518016815186\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6577130556106567\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6721093654632568\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7570217847824097\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7001845836639404\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7170883417129517\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.572660207748413\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6779130697250366\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7837092876434326\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4704598188400269\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.493690013885498\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7094224691390991\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.920799970626831\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.227926254272461\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6764440536499023\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5471034049987793\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.671701192855835\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8414475917816162\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.902266502380371\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6210148334503174\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.9145737886428833\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4882237911224365\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.0545365810394287\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.1046719551086426\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.74787437915802\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6759811639785767\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.693081259727478\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5268272161483765\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.61886465549469\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6424775123596191\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5525891780853271\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5551726818084717\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.447028636932373\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6456772089004517\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7932171821594238\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.469152808189392\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6669220924377441\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6935287714004517\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7543213367462158\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.680396318435669\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7081915140151978\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6734744310379028\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6010496616363525\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5870412588119507\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5175766944885254\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7206389904022217\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7717865705490112\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6415071487426758\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7463737726211548\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.638104796409607\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.609125018119812\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6928592920303345\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4935373067855835\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6478171348571777\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 2.02187180519104\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.659613013267517\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8780544996261597\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6645361185073853\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6152536869049072\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5784962177276611\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.568526268005371\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7181744575500488\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.712835431098938\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.764269471168518\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.579099416732788\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7414824962615967\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5736846923828125\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4528429508209229\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6679328680038452\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8170679807662964\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.491381287574768\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6924391984939575\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.2722338438034058\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7647076845169067\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7817368507385254\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5965924263000488\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5786439180374146\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8326460123062134\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6174429655075073\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.728165626525879\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7536870241165161\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.664375901222229\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5444070100784302\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6015015840530396\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6502147912979126\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8310613632202148\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8123396635055542\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6687374114990234\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.7221996784210205\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5517747402191162\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8015497922897339\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.826398253440857\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8423134088516235\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6543089151382446\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6760218143463135\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6216861009597778\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8491624593734741\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6752525568008423\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.8179974555969238\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.5888034105300903\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.6651790142059326\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4752039909362793\n",
      "torch.Size([64, 41]) torch.Size([64])\n",
      "Epoch 0, loss 1.4659748077392578\n",
      "torch.Size([64, 41]) torch.Size([64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y_batch)\n\u001b[1;32m      8\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m----> 9\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    307\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train your model\n",
    "for epoch in range(10): # number of epochs\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = nn_model(X_batch)\n",
    "        print(output.shape, y_batch.shape)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network training accuracy: 0.41600199242622593\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        output = nn_model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "nn_accuracy = correct / total\n",
    "print(f'Neural Network training accuracy: {nn_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network validation accuracy: 0.41135150323124475\n"
     ]
    }
   ],
   "source": [
    "# Validate your model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        output = nn_model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "nn_accuracy = correct / total\n",
    "print(f'Neural Network validation accuracy: {nn_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2438,  0.0368,  0.1006,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0031,  0.1986, -0.1564,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0663, -0.0284,  0.1491,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1661, -0.0798,  0.1029,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1276,  0.0789, -0.1726,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2311,  0.0249,  0.1263,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "torch.Size([64, 1049])\n",
      "tensor([ 0,  0, 15,  0, 15, 15,  3,  0, 32, 15,  0,  8,  0,  0,  7,  7,  7,  0,\n",
      "         0, 31,  0,  0,  1, 21,  0,  2, 12, 12,  1,  0, 11,  0, 18,  5,  7,  7,\n",
      "         0,  8, 21, 15,  0,  3,  0,  0, 11,  0,  0,  7,  3,  1,  0,  0, 11,  8,\n",
      "         9,  7, 15,  0,  0, 17, 25, 13, 14,  2])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in val_loader:\n",
    "    print(X_batch)\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch)\n",
    "    print(y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
